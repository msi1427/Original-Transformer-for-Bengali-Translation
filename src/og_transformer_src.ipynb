{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "og_transformer_src.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOMfYk_zfW8H"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTzjn9xBiK6_"
      },
      "source": [
        "# Self Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fuzz_KVkiKXy"
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self,embed_size,heads):\n",
        "    super(SelfAttention,self).__init__()\n",
        "    '''\n",
        "    heads : how many parts are we going to split the word embedding\n",
        "    embed_size : size of word embedding\n",
        "    '''\n",
        "    self.embed_size = embed_size\n",
        "    self.heads = heads\n",
        "    self.head_dim = embed_size // heads # integer division\n",
        "\n",
        "    assert (self.head_dim * heads == embed_size), \"embed_size needs to be divisible by heads\"\n",
        "\n",
        "    self.values = nn.Linear(self.head_dim,self.head_dim,bias=False)\n",
        "    self.keys = nn.Linear(self.head_dim,self.head_dim,bias=False)\n",
        "    self.queries = nn.Linear(self.head_dim,self.head_dim,bias=False)\n",
        "    self.fc_out = nn.Linear(heads*self.head_dim,embed_size)\n",
        "\n",
        "  def forward(self, values, keys, queries, mask):\n",
        "    '''\n",
        "    N : no. of training examples\n",
        "    '''\n",
        "    N = queries.shape[0]\n",
        "    value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
        "\n",
        "    # Split embedding into self.head pieces\n",
        "    values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "    keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "    queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
        "  \n",
        "    '''\n",
        "    queries shape: (N, query_len, heads, heads_dim)\n",
        "    keys shape: (N, key_len, heads, heads_dim)\n",
        "    score shape: (N, heads, query_len, key_len)\n",
        "    '''\n",
        "    score = torch.einsum(\"nqhd,nkhd -> nhqk\", [queries,keys]) \n",
        "    # Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.\n",
        "\n",
        "    if mask is not None:\n",
        "      score = score.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "    \n",
        "    attention = torch.softmax(score / (self.embed_size ** (1/2)), dim=3)\n",
        "\n",
        "    '''\n",
        "    attention shape: (N, heads, query_len, key_len)\n",
        "    values shape: (N, value_len, heads, heads_dim)\n",
        "    out shape: (N, query_len, heads, heads_dim)\n",
        "    key_len and value_len are always same\n",
        "    '''\n",
        "    attention = torch.einsum(\"nhql,nlhd -> nqhd\", [attention,values]).reshape(\n",
        "        N,query_len,self.heads*self.head_dim\n",
        "    )\n",
        "    \n",
        "    out = self.fc_out(attention)\n",
        "    return out\n"
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}